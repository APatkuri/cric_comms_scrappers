{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Men, Women\n",
    "cat = \"Men\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"https://scores.bcci.tv/feeds-international/internationalarchives.js\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCCI = {\"BCCI_RECENT_MATCH_URL\": f\"https://scores2.bcci.tv/getRecentMatches?platform=international&previousMatchesCount=999&filterType={cat}&loadMore=true&archieves=true\",\n",
    "\"BCCI_LIVE_MATCH_URL\": f\"https://scores2.bcci.tv/getLiveMatches?platform=international&previousMatchesCount=999&filterType={cat}&loadMore=true&archieves=true\",\n",
    "\"BCCI_UPCOMING_MATCH_URL\": f\"https://scores2.bcci.tv/getUpcomingMatches?platform=international&previousMatchesCount=999&filterType={cat}&loadMore=true&archieves=true\"}\n",
    "\n",
    "# CRICKET_AUS_COMMS = \"https://apiv2.cricket.com.au/web/views/comments?fixtureId=20755&inningNumber=2&commentType=&overLimit=21&jsconfig=eccn%3Atrue&format=json\"\n",
    "# CRICKET_AUS_MATCH = \"https://apiv2.cricket.com.au/web/fixtures/yearfilter?isCompleted=true&year=2025&limit=13&isInningInclude=true&jsconfig=eccn%3Atrue&format=json\"\n",
    "\n",
    "bcci_match_list = []\n",
    "for i in BCCI.values():\n",
    "    response = requests.get(i, timeout=100)\n",
    "    data = response.json()\n",
    "    rows = list(data.keys())\n",
    "    if(rows[0] == 'recentMatches'):\n",
    "        for j in data[list(data.keys())[-1]]:\n",
    "            bcci_match_list.extend(j)\n",
    "    else:\n",
    "        bcci_match_list.extend(data[list(data.keys())[0]])\n",
    "\n",
    "bcci_match_list = sorted(bcci_match_list, key = lambda x: x[\"MatchDate\"])\n",
    "\n",
    "with open(f\"./bcci_shot_data/{cat}/bcci_match_list.json\", 'w') as f:\n",
    "    json.dump(bcci_match_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bcci_shot_data(match_id, max_overs):\n",
    "    # match_id = 1653\n",
    "    innings = 0\n",
    "\n",
    "    if max_overs == 20 or max_overs == 50:\n",
    "        innings = 2\n",
    "    else:\n",
    "        innings = 4\n",
    "\n",
    "    match_data = []\n",
    "    for i in range(1, innings+1):\n",
    "        BCCI_COMMS_URL = f\"https://scores.bcci.tv/feeds-international/scoringfeeds/{match_id}-Innings{i}.js\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(BCCI_COMMS_URL, timeout=100)\n",
    "            response.raise_for_status()\n",
    "            data = response.text\n",
    "            # result = re.sub(r'onScoring\\((.*?)\\);', r'\\1', data)\n",
    "            result = data.replace(\"onScoring(\", \"\").replace(\");\", \"\")\n",
    "            data_json = json.loads(result)\n",
    "            match_data.append(data_json[f\"Innings{i}\"])\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 404:\n",
    "                continue\n",
    "\n",
    "    with open(f\"./bcci_shot_data/{cat}/json/{match_id}.json\", 'w') as f:\n",
    "        json.dump(match_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_json(match_id):\n",
    "    df = pd.read_json(f\"./bcci_shot_data/{cat}/json/{match_id}.json\")\n",
    "\n",
    "    dfs = []\n",
    "    for i in range(len(df['OverHistory'])):\n",
    "        dfs.append(pd.DataFrame(df['OverHistory'][i]))\n",
    "        \n",
    "    final_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # final_df.to_csv(f\"./bcci_shot_data/{match_id}.csv\")\n",
    "    final_df = final_df.drop(columns=['BallID', 'BallUniqueID', 'StrikerID', 'NonStrikerID', 'BowlerID', \n",
    "                                    'VideoFile', 'NewCommentry', 'Commentry', 'UPDCommentry', 'OutBatsManID',\n",
    "                                    'HatCheck', 'CommentStrikers', 'OverName', 'CommentOver', 'RunsText'])\n",
    "\n",
    "\n",
    "    final_df = final_df[final_df['ActualBallNo'].str.strip() != '']\n",
    "    final_df.to_csv(f\"./bcci_shot_data/{cat}/csv/{match_id}.csv\")\n",
    "    final_df.columns.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(bcci_match_list)\n",
    "temp_df = temp_df.drop_duplicates(subset=['MatchID'], keep='first', inplace=False)\n",
    "\n",
    "live_data_file_name = f\"./bcci_shot_data/{cat}/live_data_file_name.txt\"\n",
    "try:\n",
    "    with open(live_data_file_name, 'r') as file:\n",
    "        existing_ids = set(line.strip() for line in file)\n",
    "except FileNotFoundError:\n",
    "    existing_ids = set()\n",
    "\n",
    "data_temp_df = temp_df[temp_df['MatchStatus'] == 'Post']\n",
    "live_data_temp_df = temp_df[temp_df['MatchStatus'] == 'Live']\n",
    "\n",
    "for match_id ,max_overs in zip(data_temp_df['MatchID'], data_temp_df['MATCH_NO_OF_OVERS']):\n",
    "    temp_file_str = f\"./bcci_shot_data/{cat}/csv/{match_id}.csv\"\n",
    "\n",
    "    if temp_file_str not in glob.glob(f\"./bcci_shot_data/{cat}/csv/*.csv\"):\n",
    "        get_bcci_shot_data(match_id, max_overs)\n",
    "        csv_to_json(match_id)\n",
    "\n",
    "    if str(match_id) in existing_ids:\n",
    "        get_bcci_shot_data(match_id, max_overs)\n",
    "        csv_to_json(match_id)\n",
    "        existing_ids.remove(str(match_id))\n",
    "\n",
    "# live_match_list = []\n",
    "for match_id ,max_overs in zip(live_data_temp_df['MatchID'], live_data_temp_df['MATCH_NO_OF_OVERS']):\n",
    "    # live_match_list.append(match_id)\n",
    "    if match_id not in existing_ids:\n",
    "        existing_ids.add(match_id)\n",
    "\n",
    "    get_bcci_shot_data(match_id, max_overs)\n",
    "    csv_to_json(match_id)\n",
    "    \n",
    "with open(live_data_file_name, 'w') as file:\n",
    "    file.write('\\n'.join(str(id) for id in existing_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22274/364400779.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  temp_df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "temp_df.fillna('', inplace=True)\n",
    "temp_df = temp_df.drop(columns=['PreMatchCommentary', 'PostMatchCommentary', 'innings'], errors='ignore')\n",
    "temp_df.to_csv(f'./bcci_shot_data/{cat}/bcci_match_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been concatenated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = f'./bcci_shot_data/{cat}/csv'  # Path to the 'csv' directory\n",
    "\n",
    "# List of numbers (in the order you want the CSV files to be combined)\n",
    "temp_list = temp_df['MatchID'].tolist()\n",
    "\n",
    "# List of all CSV files in the folder\n",
    "csv_files = list(f\"{num}.csv\" for num in temp_list)  # Set for fast lookup\n",
    "\n",
    "# Read and concatenate the CSVs\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if os.path.exists(file_path):  # Only process if the file exists\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if df_list:\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    # Save the result to a new CSV\n",
    "    final_df.to_csv(f'./bcci_shot_data/{cat}/combined_shot_data.csv', index=False)\n",
    "    print('CSV files have been concatenated successfully!')\n",
    "else:\n",
    "    print('No CSV files were found to combine.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.bcci.tv/events/183/border-gavaskar-trophy-2024-25/match/1652/4th-test\n",
    "\n",
    "# temp_df = pd.DataFrame(bcci_match_list)\n",
    "hawkeye_file_name = f\"./bcci_shot_data/{cat}/hawkeyeid_matchid.txt\"\n",
    "try:\n",
    "    with open(hawkeye_file_name, 'r') as file:\n",
    "        hawkeye_ids = [tuple(map(int, line.strip().split(', '))) for line in file]\n",
    "except FileNotFoundError:\n",
    "    hawkeye_ids = []\n",
    "\n",
    "temp_df = pd.read_json(f\"./bcci_shot_data/{cat}/bcci_match_list.json\")\n",
    "india_match_df = temp_df[(temp_df['HomeTeamName'] == 'India') | (temp_df['HomeTeamName'] == 'India (Women)')]\n",
    "for c_id, c_name, m_id, m_order in zip(india_match_df['CompetitionID'], india_match_df['CompetitionName'], india_match_df['MatchID'], india_match_df['MatchOrder']):\n",
    "\n",
    "    c_name_new = re.sub(r'\\s+', '-', c_name.lower())\n",
    "    m_order_new = re.sub(r'\\s+', '-', m_order.lower())\n",
    "\n",
    "    match_center_str = f\"https://www.bcci.tv/events/{c_id}/{c_name_new}/match/{m_id}/{m_order_new}\"\n",
    "\n",
    "    response = requests.get(match_center_str, timeout=30)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = soup.find_all('embed')\n",
    "    if len(text) != 0:\n",
    "        hawkurl = text[0].get('src', '')\n",
    "        hawkid = int(hawkurl.split(\"matchId=\")[-1])\n",
    "\n",
    "        if (m_id, hawkid) not in hawkeye_ids:\n",
    "            hawkeye_ids.append((m_id, hawkid))\n",
    "\n",
    "# print(hawkeye_ids)\n",
    "with open(hawkeye_file_name, 'w') as file:\n",
    "    for i, j in hawkeye_ids:\n",
    "        file.write(f\"{i}, {j}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCCI_ARCHIVES_URL = \"https://scores.bcci.tv/feeds-international/internationalarchives.js\"\n",
    "response = requests.get(BCCI_ARCHIVES_URL, timeout=30)\n",
    "data = response.text\n",
    "result = data.replace(\"oncomptetion(\", \"\").replace(\");\", \"\")\n",
    "final = json.loads(result)\n",
    "archive_df = pd.DataFrame(final['competition'])\n",
    "# archive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_shot_data_match_list = archive_df[archive_df['completed'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8    ENGLAND LIONS TOUR OF INDIA\n",
      "Name: CompetitionName, dtype: object\n",
      "11    ENGLAND LIONS TOUR OF INDIA MULTIDAY WARM UP GAME\n",
      "Name: CompetitionName, dtype: object\n",
      "21    ENGLAND A WOMENS TOUR OF INDIA T20 SERIES\n",
      "Name: CompetitionName, dtype: object\n",
      "24    QUADRANGULAR MENS U19 ONE DAY SERIES\n",
      "Name: CompetitionName, dtype: object\n",
      "51    INDIA A IN BANGLADESH MULTI DAY SERIES\n",
      "Name: CompetitionName, dtype: object\n",
      "52    NEW ZEALAND WM U19 IN INDIA T20 SERIES\n",
      "Name: CompetitionName, dtype: object\n",
      "55    WEST INDIES WM U19 VS NEW ZEALAND WM U19 IN INDIA\n",
      "Name: CompetitionName, dtype: object\n",
      "62    NEW ZEALAND A TOUR OF INDIA\n",
      "Name: CompetitionName, dtype: object\n",
      "63    NEW ZEALAND A TOUR OF INDIA\n",
      "Name: CompetitionName, dtype: object\n",
      "92    AUSTRALIA WOMEN V INDIA WOMEN 2021\n",
      "Name: CompetitionName, dtype: object\n",
      "95    ENGLAND WOMEN V INDIA WOMEN 2021\n",
      "Name: CompetitionName, dtype: object\n",
      "98    INDIA WOMEN V SOUTH AFRICA WOMEN 2021\n",
      "Name: CompetitionName, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# def bcci_archive_data(comp_id):\n",
    "bcci_archive_data = []\n",
    "for comp_id in archive_shot_data_match_list['CompetitionID']:\n",
    "    try:\n",
    "        BCCI_SCORING_FEEDS_URL = f\"https://scores.bcci.tv/feeds-international/scoringfeeds/{comp_id}-matchschedule.js\"\n",
    "        response = requests.get(BCCI_SCORING_FEEDS_URL, timeout=30)\n",
    "        data = response.text\n",
    "        result = data.replace(\"MatchSchedule(\", \"\").replace(\");\", \"\")\n",
    "        final = json.loads(result)\n",
    "        bcci_archive_data.extend(final['Matchsummary'])\n",
    "    except:\n",
    "        print(archive_df[archive_df['CompetitionID'] == comp_id]['CompetitionName'])\n",
    "        # BCCI_SCORING_FEEDS_URL = f\"https://scores.bcci.tv/feeds-international/{comp_id}-matchschedule.js?callback=MatchSchedule&_=1738899249298\"\n",
    "        # response = requests.get(BCCI_SCORING_FEEDS_URL, timeout=30)\n",
    "        # data = response.text\n",
    "        # result = data.replace(\"MatchSchedule(\", \"\").replace(\");\", \"\")\n",
    "        # final = json.loads(result)\n",
    "        # bcci_archive_data.extend(final['Matchsummary'])\n",
    "    # return final['Matchsummary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      5\n",
       "1      1\n",
       "2      2\n",
       "3      1\n",
       "4      9\n",
       "      ..\n",
       "365    2\n",
       "366    2\n",
       "367    2\n",
       "368    2\n",
       "369    2\n",
       "Name: HomeTeamID, Length: 370, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcci_archive_df = pd.DataFrame(bcci_archive_data)\n",
    "bcci_archive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(data):\n",
    "#     df = pd.json_normalize(data['inning'], 'overs', ['id', 'fixtureId', 'inningNumber', 'battingTeamId', 'bowlingTeamId'], record_prefix='over_')\n",
    "#     # df1 = pd.json_normalize(df['balls'])\n",
    "#     # df = df.iloc[::-1].reset_index(drop=True)\n",
    "#     if(len(df) > 0):\n",
    "#         df = df.drop(index=df.index[-1])\n",
    "#         df = df.explode('over_balls', ignore_index=True)\n",
    "#         df1 = pd.json_normalize(df['over_balls'])\n",
    "#         df.drop(columns=['over_balls'], inplace=True)\n",
    "#         df2 = pd.concat([df, df1], axis=1)\n",
    "#         # df2['ball_comments'] = df2['comments'].apply(lambda x: ', '.join([comment['comments'] for comment in x]) if isinstance(x, list) else '')\n",
    "#         # df2\n",
    "\n",
    "#         # df3 = pd.json_normalize(df2)\n",
    "#         df2 = df2.iloc[::-1].reset_index(drop=True)\n",
    "#         df2.drop(columns=['comments'], inplace=True)\n",
    "#     else:\n",
    "#         df2 = pd.DataFrame()\n",
    "\n",
    "#     return df2\n",
    "#     # df2.to_csv('./temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main_func(matchid):\n",
    "#     comms_df = pd.DataFrame()\n",
    "#     for i in range(1, 3):\n",
    "#         CRICKET_AUS_COMMS = f\"https://apiv2.cricket.com.au/web/views/comments?fixtureId={matchid}&inningNumber={i}&commentType=&overLimit=499&jsconfig=eccn%3Atrue&format=json\"\n",
    "#         response = requests.get(CRICKET_AUS_COMMS, timeout=100)\n",
    "#         data = response.json()\n",
    "#         df = preprocess(data)\n",
    "#         comms_df = pd.concat([comms_df, df], ignore_index=True)\n",
    "\n",
    "#     comms_df.to_csv(f\"./{matchid}.csv\", index=False)\n",
    "#     # with open(\"./temp.json\", \"w\") as f:\n",
    "#     #     json.dump(comms_data, f, indent=4)\n",
    "#     # comms_data.extend(data['inning'])\n",
    "#     # df = pd.read_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_func(11291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['416', '13288607242335'],\n",
       " ['417', '13288866796418'],\n",
       " ['418', '13289039898867'],\n",
       " ['419', '13289490448693'],\n",
       " ['420', '13289663853284'],\n",
       " ['421', '13289837179102'],\n",
       " ['424', '13290181311355'],\n",
       " ['425', '13290354143841'],\n",
       " ['426', '13290440625168'],\n",
       " ['422', '13290838587046'],\n",
       " ['427', '13299253690057'],\n",
       " ['428', '13299512638233'],\n",
       " ['429', '13299685274523'],\n",
       " ['430', '13299944840163'],\n",
       " ['431', '13300117631732'],\n",
       " ['636', '13308153194853'],\n",
       " ['637', '13308421662088'],\n",
       " ['638', '13308584619431'],\n",
       " ['639', '13308844210044'],\n",
       " ['640', '13309189373728'],\n",
       " ['641', '13309362654636'],\n",
       " ['642', '13309524050930'],\n",
       " ['643', '13309774466666'],\n",
       " ['644', '13309949672842'],\n",
       " ['726', '13317224945965'],\n",
       " ['727', '13317397841340'],\n",
       " ['728', '13317568537021'],\n",
       " ['729', '13317810030358'],\n",
       " ['730', '13317980527734'],\n",
       " ['731', '13318241938233'],\n",
       " ['732', '13318499127340'],\n",
       " ['744', '13318760641787'],\n",
       " ['733', '13319017446902'],\n",
       " ['734', '13319298330833'],\n",
       " ['735', '13319470867371'],\n",
       " ['736', '13319730207544'],\n",
       " ['740', '13320387790686'],\n",
       " ['741', '13321060148049'],\n",
       " ['742', '13322095374323'],\n",
       " ['743', '13322787102478'],\n",
       " ['737', '13323512583821'],\n",
       " ['738', '13323685769288'],\n",
       " ['739', '13323943940433'],\n",
       " ['1162', '13339842860997'],\n",
       " ['1163', '13340014726352'],\n",
       " ['1164', '13340274172117'],\n",
       " ['1165', '13345218219467'],\n",
       " ['1166', '13345477366471'],\n",
       " ['1167', '13345650347817'],\n",
       " ['1168', '13345909270840'],\n",
       " ['1169', '13346082443028'],\n",
       " ['1170', '13349451671648'],\n",
       " ['1171', '13349711578682'],\n",
       " ['1172', '13349970611714'],\n",
       " ['1173', '13350627225448'],\n",
       " ['1174', '13351318403527'],\n",
       " ['1175', '13352441696389'],\n",
       " ['1176', '13353132713745'],\n",
       " ['1177', '13354256307986'],\n",
       " ['1565', '13371190451590'],\n",
       " ['1566', '13371885097789'],\n",
       " ['1567', '13372693269120'],\n",
       " ['1568', '13372954025963'],\n",
       " ['1569', '13373212587628'],\n",
       " ['1570', '13373610336646'],\n",
       " ['1571', '13374214432635'],\n",
       " ['1572', '13374905688603'],\n",
       " ['1573', '13382026073188'],\n",
       " ['1574', '13382026073188'],\n",
       " ['1575', '13382544438383'],\n",
       " ['1576', '13382802067879'],\n",
       " ['1577', '13382975708080'],\n",
       " ['1578', '13383301309571']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hawk_pair = []\n",
    "with open(\"./bcci_shot_data/Men/hawkeyeid_matchid.txt\") as f:\n",
    "    for l in f:\n",
    "        split_values = [value.strip() for value in l.strip().split(',')]\n",
    "        hawk_pair.append(split_values)\n",
    "\n",
    "hawk_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted data has been saved to 'sorted_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Read the CSV file and store the data as a list of rows\n",
    "data = []\n",
    "\n",
    "with open('./hawkid_espnid.csv', 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        # Convert values to integers (assuming the file has two columns)\n",
    "        data.append([int(row[0]), int(row[1])])  # Store as a list [first_value, second_value]\n",
    "\n",
    "# Sort the list based on the second value (index 1 of each row)\n",
    "sorted_data = sorted(data, key=lambda x: x[0])\n",
    "\n",
    "# Write the sorted data back to a new CSV file\n",
    "with open('./hawkid_espnid_new.csv.csv', 'w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for row in sorted_data:\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(\"Sorted data has been saved to 'sorted_file.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Base URL without matchId\n",
    "# base_url = \"https://polls.iplt20.com/?entity_matchId=74795&matchId=\"\n",
    "\n",
    "# # Define your start and end times\n",
    "# start_date = datetime(2024, 1, 1, 12, 0)  # Example start time\n",
    "# end_date = datetime(2024, 1, 1, 12, 30)  # Example end time (half an hour later)\n",
    "\n",
    "# # Generate timestamps for the range\n",
    "# current_time = start_date\n",
    "# time_increment = timedelta(seconds=1)  # Increment by 1 second\n",
    "\n",
    "\n",
    "# while current_time <= end_date:\n",
    "#     # Convert to Unix timestamp (seconds since epoch)\n",
    "#     match_id = int(current_time.timestamp())\n",
    "#     url = f\"{base_url}{match_id}\"\n",
    "\n",
    "#     try:\n",
    "#         # Send a GET request\n",
    "#         response = requests.get(url, timeout=10)\n",
    "\n",
    "#         # Log the response\n",
    "#         print(f\"URL: {url}\")\n",
    "#         print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "#         if response.status_code == 200:\n",
    "#             print(f\"Response: {response.text}\\n\")\n",
    "#         else:\n",
    "#             print(\"Non-200 response received.\\n\")\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Request failed for URL {url}: {e}\\n\")\n",
    "\n",
    "#     # Increment time\n",
    "#     current_time += time_increment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
