{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Men, Women\n",
    "cat = \"Women\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"https://scores.bcci.tv/feeds-international/internationalarchives.js\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCCI = {\"BCCI_RECENT_MATCH_URL\": f\"https://scores2.bcci.tv/getRecentMatches?platform=international&previousMatchesCount=999&filterType={cat}&loadMore=true&archieves=true\",\n",
    "\"BCCI_LIVE_MATCH_URL\": f\"https://scores2.bcci.tv/getLiveMatches?platform=international&previousMatchesCount=999&filterType={cat}&loadMore=true&archieves=true\",\n",
    "\"BCCI_UPCOMING_MATCH_URL\": f\"https://scores2.bcci.tv/getUpcomingMatches?platform=international&previousMatchesCount=999&filterType={cat}&loadMore=true&archieves=true\"}\n",
    "\n",
    "CRICKET_AUS_COMMS = \"https://apiv2.cricket.com.au/web/views/comments?fixtureId=20755&inningNumber=2&commentType=&overLimit=21&jsconfig=eccn%3Atrue&format=json\"\n",
    "# CRICKET_AUS_MATCH = \"https://apiv2.cricket.com.au/web/fixtures/yearfilter?isCompleted=true&year=2025&limit=13&isInningInclude=true&jsconfig=eccn%3Atrue&format=json\"\n",
    "\n",
    "bcci_match_list = []\n",
    "for i in BCCI.values():\n",
    "    response = requests.get(i, timeout=100)\n",
    "    data = response.json()\n",
    "    rows = list(data.keys())\n",
    "    if(rows[0] == 'recentMatches'):\n",
    "        for j in data[list(data.keys())[-1]]:\n",
    "            bcci_match_list.extend(j)\n",
    "    else:\n",
    "        bcci_match_list.extend(data[list(data.keys())[0]])\n",
    "\n",
    "bcci_match_list = sorted(bcci_match_list, key = lambda x: x[\"MatchDate\"])\n",
    "\n",
    "with open(f\"./bcci_shot_data/{cat}/bcci_match_list.json\", 'w') as f:\n",
    "    json.dump(bcci_match_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bcci_shot_data(match_id, max_overs):\n",
    "    # match_id = 1653\n",
    "    innings = 0\n",
    "\n",
    "    if max_overs == 20 or max_overs == 50:\n",
    "        innings = 2\n",
    "    else:\n",
    "        innings = 4\n",
    "\n",
    "    match_data = []\n",
    "    for i in range(1, innings+1):\n",
    "        BCCI_COMMS_URL = f\"https://scores.bcci.tv/feeds-international/scoringfeeds/{match_id}-Innings{i}.js\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(BCCI_COMMS_URL, timeout=100)\n",
    "            response.raise_for_status()\n",
    "            data = response.text\n",
    "            # result = re.sub(r'onScoring\\((.*?)\\);', r'\\1', data)\n",
    "            result = data.replace(\"onScoring(\", \"\").replace(\");\", \"\")\n",
    "            data_json = json.loads(result)\n",
    "            match_data.append(data_json[f\"Innings{i}\"])\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 404:\n",
    "                continue\n",
    "\n",
    "    with open(f\"./bcci_shot_data/{cat}/json/{match_id}.json\", 'w') as f:\n",
    "        json.dump(match_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_json(match_id):\n",
    "    df = pd.read_json(f\"./bcci_shot_data/{cat}/json/{match_id}.json\")\n",
    "\n",
    "    dfs = []\n",
    "    for i in range(len(df['OverHistory'])):\n",
    "        dfs.append(pd.DataFrame(df['OverHistory'][i]))\n",
    "        \n",
    "    final_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # final_df.to_csv(f\"./bcci_shot_data/{match_id}.csv\")\n",
    "    final_df = final_df.drop(columns=['BallID', 'BallUniqueID', 'StrikerID', 'NonStrikerID', 'BowlerID', \n",
    "                                    'VideoFile', 'NewCommentry', 'Commentry', 'UPDCommentry', 'OutBatsManID',\n",
    "                                    'HatCheck', 'CommentStrikers', 'OverName', 'CommentOver', 'RunsText'])\n",
    "\n",
    "\n",
    "    final_df = final_df[final_df['ActualBallNo'].str.strip() != '']\n",
    "    final_df.to_csv(f\"./bcci_shot_data/{cat}/csv/{match_id}.csv\")\n",
    "    final_df.columns.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(bcci_match_list)\n",
    "temp_df = temp_df.drop_duplicates(subset=['MatchID'], keep='first', inplace=False)\n",
    "\n",
    "data_temp_df = temp_df[temp_df['MatchStatus'] == 'Post']\n",
    "\n",
    "# for match_id ,max_overs in zip(data_temp_df['MatchID'], data_temp_df['MATCH_NO_OF_OVERS']):\n",
    "#     get_bcci_shot_data(match_id, max_overs)\n",
    "#     csv_to_json(match_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43660/3674046442.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  temp_df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "temp_df.fillna('', inplace=True)\n",
    "temp_df = temp_df.drop(columns=['PreMatchCommentary', 'PostMatchCommentary'])\n",
    "temp_df.to_csv(f'./bcci_shot_data/{cat}/bcci_match_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been concatenated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = f'./bcci_shot_data/{cat}/csv'  # Path to the 'csv' directory\n",
    "\n",
    "# List of numbers (in the order you want the CSV files to be combined)\n",
    "temp_list = temp_df['MatchID'].tolist()\n",
    "\n",
    "# List of all CSV files in the folder\n",
    "csv_files = list(f\"{num}.csv\" for num in temp_list)  # Set for fast lookup\n",
    "\n",
    "# Read and concatenate the CSVs\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if os.path.exists(file_path):  # Only process if the file exists\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if df_list:\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    # Save the result to a new CSV\n",
    "    final_df.to_csv(f'./bcci_shot_data/{cat}/combined_shot_data.csv', index=False)\n",
    "    print('CSV files have been concatenated successfully!')\n",
    "else:\n",
    "    print('No CSV files were found to combine.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    df = pd.json_normalize(data['inning'], 'overs', ['id', 'fixtureId', 'inningNumber', 'battingTeamId', 'bowlingTeamId'], record_prefix='over_')\n",
    "    # df1 = pd.json_normalize(df['balls'])\n",
    "    # df = df.iloc[::-1].reset_index(drop=True)\n",
    "    if(len(df) > 0):\n",
    "        df = df.drop(index=df.index[-1])\n",
    "        df = df.explode('over_balls', ignore_index=True)\n",
    "        df1 = pd.json_normalize(df['over_balls'])\n",
    "        df.drop(columns=['over_balls'], inplace=True)\n",
    "        df2 = pd.concat([df, df1], axis=1)\n",
    "        # df2['ball_comments'] = df2['comments'].apply(lambda x: ', '.join([comment['comments'] for comment in x]) if isinstance(x, list) else '')\n",
    "        # df2\n",
    "\n",
    "        # df3 = pd.json_normalize(df2)\n",
    "        df2 = df2.iloc[::-1].reset_index(drop=True)\n",
    "        df2.drop(columns=['comments'], inplace=True)\n",
    "    else:\n",
    "        df2 = pd.DataFrame()\n",
    "\n",
    "    return df2\n",
    "    # df2.to_csv('./temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(matchid):\n",
    "    comms_df = pd.DataFrame()\n",
    "    for i in range(1, 3):\n",
    "        CRICKET_AUS_COMMS = f\"https://apiv2.cricket.com.au/web/views/comments?fixtureId={matchid}&inningNumber={i}&commentType=&overLimit=499&jsconfig=eccn%3Atrue&format=json\"\n",
    "        response = requests.get(CRICKET_AUS_COMMS, timeout=100)\n",
    "        data = response.json()\n",
    "        df = preprocess(data)\n",
    "        comms_df = pd.concat([comms_df, df], ignore_index=True)\n",
    "\n",
    "    comms_df.to_csv(f\"./{matchid}.csv\", index=False)\n",
    "    # with open(\"./temp.json\", \"w\") as f:\n",
    "    #     json.dump(comms_data, f, indent=4)\n",
    "    # comms_data.extend(data['inning'])\n",
    "    # df = pd.read_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_func(11291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Base URL without matchId\n",
    "# base_url = \"https://polls.iplt20.com/?entity_matchId=74795&matchId=\"\n",
    "\n",
    "# # Define your start and end times\n",
    "# start_date = datetime(2024, 1, 1, 12, 0)  # Example start time\n",
    "# end_date = datetime(2024, 1, 1, 12, 30)  # Example end time (half an hour later)\n",
    "\n",
    "# # Generate timestamps for the range\n",
    "# current_time = start_date\n",
    "# time_increment = timedelta(seconds=1)  # Increment by 1 second\n",
    "\n",
    "\n",
    "# while current_time <= end_date:\n",
    "#     # Convert to Unix timestamp (seconds since epoch)\n",
    "#     match_id = int(current_time.timestamp())\n",
    "#     url = f\"{base_url}{match_id}\"\n",
    "\n",
    "#     try:\n",
    "#         # Send a GET request\n",
    "#         response = requests.get(url, timeout=10)\n",
    "\n",
    "#         # Log the response\n",
    "#         print(f\"URL: {url}\")\n",
    "#         print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "#         if response.status_code == 200:\n",
    "#             print(f\"Response: {response.text}\\n\")\n",
    "#         else:\n",
    "#             print(\"Non-200 response received.\\n\")\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Request failed for URL {url}: {e}\\n\")\n",
    "\n",
    "#     # Increment time\n",
    "#     current_time += time_increment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
